#include "platform.h"
#include "object.h"
#include "atomics.h"
#include "asserts.h"
#include "pool.h"
#include "share.h"

// These are the "overhead" fields that are allocated as a prefix to each
// `share` object created by the runtime.
typedef struct share_header_t {
  // For a "definitely live" object (whose ref_count is definitely > than 0),
  // This `link` is a pointer to the `share_state_t` that allocated the object
  // (and thus, the one which should free it when ready).
  //
  // For a "possibly dead" object (whose ref_count dropped to 0 at some point),
  // this `link` is a pointer to the next object in the "possibly dead" list,
  // if any. If there is no next object in the list, this `link` is NULL.
  VECS_ATOMIC(void*) link;

  // The reference count of the object.
  //
  // This gets incremented (by a call to `vecs_share_inc_ref` generated by
  // the compiler) whenever a field somewhere stores a reference to this object.
  // It gets decremented (by a call to `vecs_share_dec_ref` generated by the
  // compiler) whenever a field somewhere removes a reference to this object.
  //
  // When decremented to 0, the object is added to the "possibly dead" list
  // for the owning `share_state_t` to clean up later (at a time when all
  // threads definitely are not in flight, adding/removing references).
  VECS_ATOMIC(size_t) ref_count;
} share_header_t;


// An allocated share object consists of a header, followed by the object.
// Note that the object itself gets some arbitrary data fields appended after.
typedef struct share_object_t {
  share_header_t header;
  vecs_object_t object;
} share_object_t;

// The thread-local state for the share part of the runtime (one per thread).
typedef struct share_state_t {
  // This will be initialized to a value that can NEVER match a type descriptor
  // pointer value: the maximum value for `size_t` (i.e. `SIZE_MAX`).
  //
  // This will allow us to reliably tell whether a given `link` pointer is
  // pointing to a share_state_t (with SIZE_MAX as the first word) vs a
  // `share_object_t` (with a valid type descriptor pointer as the first word).
  size_t marker_bytes;

  // The list of objects which at one time had their reference count drop to 0.
  // When the list is empty, this pointer is NULL.
  VECS_ATOMIC(share_object_t*) possibly_dead_list_head;

  // TODO: PERF: As an optional optimization for list append operations, we can
  // also keep a pointer to an object that is "probably at or near" the tail
  // of the list. It would not be guaranteed to be the final tail, but it can
  // help keep the append cost low, by getting us close to the end before we
  // start traversing the links of the list one-by-one.
  // However, this will need additional thought and testing to ensure
  // correctness of the optimization, so we're leaving it out for now.

  // The list of objects that need to have their finalizers run,
  // and then be deallocated. When the list is empty, this pointer is NULL.
  //
  // This doesn't need to be an atomic pointer, because we will only touch
  // this list from the thread that owns this `share_state_t`.
  share_object_t* needs_fini_list_head;
} share_state_t;

static __thread share_state_t local_share_state = {SIZE_MAX, NULL};

#ifdef VECS_ASSERTS_ENABLED
static __thread bool debug_local_reclaim_in_progress = false;
static VECS_ATOMIC(size_t) debug_reclaim_threads_in_progress = 0;
#endif


// Get the share object pointer for a given object pointer.
//
// This is just pointer math: the start of the share object's memory is
// a few words "backwards" from the start of the main object's memory
// (because the share object requires an additional header, but this is
// hidden from the program, which just uses a pointer to the main object,
// because the program shouldn't need to deal with the complexity of
// different offsets for different kinds of object allocations).
static share_object_t* get_share_object(vecs_object_t* object) {
  VECS_ASSERT(object != NULL);

  return (share_object_t*)(((char*)object) - sizeof(share_header_t));
}


// Free the memory associated with the given object
// (including the extra memory that was allocated for the `share_header_t`).
//
// Note that this should only be called after finalizers have run,
// for any object whose type has a finalizer function.
static void free_object(share_object_t* share_object) {
  vecsint_pool_free_size(
    // TODO: PERF: precalculated pool index in the type descriptor
    sizeof(share_object_t) + share_object->object.type->data_size,
    share_object
  );
}


// Return true if the given link pointer indicates a "possibly dead" object.
//
// WARNING: Remember that the link pointer could concurrently change while
// you're doing this check, so be sure to use appropriate atomic operations
// to ensure that this check is still valid by the end of what you're doing.
static bool is_link_possibly_dead(const void* link) {
  // If NULL, the object is the last item in the "possibly dead" list
  if (link == NULL) return true;

  // If pointing to correct marker bytes, the object is "definitely live".
  if (((share_state_t*)link)->marker_bytes == SIZE_MAX) return false;

  // Otherwise, the object is in the middle of the "possibly dead" list.
  return false;
}


// Return true if the given share object is *actually* a baked object instead.
//
// Within the program, a baked object can be seamlessly "treated as" a
// share object, but it is actually in static program memory,
// so we can't do any mutating operations on it, like we do with share objects.
static bool is_share_object_actually_baked(const share_object_t* share_object) {
  // We use a special marker value in the `ref_count` field to indicate
  // that the object is actually a baked object, not a share object.
  //
  // We can use relaxed memory order to check the reference count,
  // (as opposed to stronger memory order semantics like acquire/release),
  // because we have an invariant that the reference count of a baked object
  // will never be modified, and that the reference count of a share object
  // will never be incremented high enough to "look like" a baked object.
  return SIZE_MAX == atomic_load_explicit(
    &share_object->header.ref_count, memory_order_relaxed
  );
}


// For an object which itself has been marked as "possibly dead", this function
// finishes the process by appending it to the "possibly dead" list.
//
// This is mainly meant for use by the `set_possibly_dead` function that
// initiates this process properly, but it's also called by the allocation
// function for a new object (which is guaranteed to be exclusively held
// and already initialized in a "possibly dead" state).
static void append_to_possibly_dead_list(
  share_object_t* object, share_state_t* share_state
) {
  VECS_ASSERT(object != NULL);
  VECS_ASSERT(share_state != NULL);
  VECS_ASSERT(
    // Nothing should be getting marked as newly dead during a reclaim phase.
    0 == atomic_load_explicit(
      &debug_reclaim_threads_in_progress, memory_order_acq_rel
    )
  );

  // This is a singly linked list with multiple threads potentially adding
  // to the tail concurrently, because multiple objects allocated from
  // a common thread may all be getting concurrently marked by parallel threads.
  // However, items will never be removed from the list concurrently,
  // because we'll never call this function during a reclaim phase.
  //
  // Therefore we can treat it as an immutable append-only log of objects,
  // meaning we can keep doing load/CAS ops to add to the tail until we succeed.
  //
  // And at this point we've claimed the right/responsibility to update the
  // state of this object, so we NEED to loop until we successfully add it.
  share_object_t* tail = atomic_load_explicit(
    &share_state->possibly_dead_list_head, memory_order_acquire
  );
  while (true) {
    // If the list is empty, we can just set the object as the new head.
    if (tail == NULL) {
      bool success = atomic_compare_exchange_weak_explicit(
        &share_state->possibly_dead_list_head, &tail, object,
        memory_order_acq_rel, memory_order_acquire
      );
      if (success) break;

      // Whoops, someone else already added stuff to the list. Let's get the
      // new tail pointer so we can start traversing to its end.
      tail = atomic_load_explicit(
        &share_state->possibly_dead_list_head, memory_order_acquire
      );

      // Even though it's possible the tail we just loaded is distinct from
      // the tail that the CAS operation above saw, we know that the tail
      // isn't NULL, because nothing can remove items from the list when
      // not in a reclaim phase, and we're not currently in a reclaim phase.
      VECS_ASSERT(tail != NULL);

      // Fall through to the rest of the loop, where we traverse the list.
    }

    // Let's see what's next in the list.
    share_object_t* next_tail = atomic_load_explicit(
      &tail->header.link, memory_order_acquire
    );

    // If this is the end of the list, we can try to add the object here.
    if (next_tail == NULL) {
      bool success = atomic_compare_exchange_weak_explicit(
        &tail->header.link, (void**)&next_tail, object,
        memory_order_acq_rel, memory_order_acquire
      );
      if (success) break;

      // Whoops, someone else already added stuff to the list. Let's get the
      // new tail pointer so we can continue traversing to its end.
      tail = atomic_load_explicit(
        &tail->header.link, memory_order_acquire
      );

      // Even though it's possible the tail we just loaded is distinct from
      // the tail that the CAS operation above saw, we know that the tail
      // isn't NULL, because nothing can remove items from the list when
      // not in a reclaim phase, and we're not currently in a reclaim phase.
      VECS_ASSERT(tail != NULL);

      // Start back from the top of the loop with this new tail pointer.
      continue;
    }

    // If we get here, it means there's another item to traverse.
    // So we update the tail pointer and keep looping.
    // We'll eventually reach the end.
    tail = next_tail;
  }
  // At this point, the object has been successfully added to the list
  // (that's the only condition that breaks the above loop).

  return;
}


// For the given object, do the relevant bookkeeping to mark it as
// "possibly dead", if it's not already marked as such already.
//
// This usually means it has a reference count of zero, but that's not
// guaranteed, because the reference count can be modified independently,
// including potential further modifications after it has been marked -
// such cases are all commonplace in a well-behaved program, because
// we are only counting durable references - not in-flight references.
static void set_possibly_dead(share_object_t* object) {
  VECS_ASSERT(object != NULL);
  VECS_ASSERT(
    // Nothing should be getting marked as newly dead during a reclaim phase.
    0 == atomic_load_explicit(
      &debug_reclaim_threads_in_progress, memory_order_acq_rel
    )
  );

  // Get the current `link` pointer from the object.
  void* link = atomic_load_explicit(
    &object->header.link, memory_order_acquire
  );

  // If the object is already "possibly dead", we don't need to do anything.
  // It will remain "possibly dead" until the reclaiming function gets to it,
  // so we don't need to worry about `link` having changed since we loaded.
  // (the reclaiming function will not run concurrently with this function).
  if (is_link_possibly_dead(link)) return;

  // We're going to set the object as "possibly dead", and it's going to be
  // the latest item in whichever "possibly dead" list it is added to.
  //
  // So we need to set the `link` pointer to NULL.
  // We need to do this before we add the object to the list, because
  // otherwise, some other thread might race us to do so.
  // Setting the `link` pointer to NULL will make `is_link_possibly_dead`
  // return true for any thread that checks it after this point, preventing
  // the race and giving us exclusive control for now over updating its state.
  bool did_claim_right_to_update = atomic_compare_exchange_weak_explicit(
    &object->header.link, &link, NULL,
    memory_order_acq_rel, memory_order_acquire
  );
  if (!did_claim_right_to_update) {
    // If we failed to claim the right to update this object's state,
    // that's no problem - it just means some other thread is doing the
    // work we were about to do. There's no other reason why this link
    // field would have changed concurrently, apart from a race to the same
    // "mark as possibly dead" semantics that we're trying to do here.
    // This "failure" is just a success of another thread doing the same thing.
    return;
  }

  // Given that the link comes from an object that was marked as definitely
  // alive, we know that the link points to the owning `share_state_t`
  // (which is not necessarily the same as the `local_share_state`).
  share_state_t* share_state = (share_state_t*)link;

  // Now what's left for us is to add the object to the "possibly dead" list.
  return append_to_possibly_dead_list(object, share_state);
}


///
// Public API (see header for docs)


vecs_object_t* vecs_share_alloc(const vecs_type_t* type) {
  VECS_ASSERT(type != NULL);

  // Allocate enough space for the share object and the object's data fields.
  share_object_t* share_object = (share_object_t*)vecsint_pool_alloc_size(
    sizeof(share_object_t) + type->data_size
  );
  VECS_ASSERT(share_object != NULL);

  // Initialize the header fields.
  atomic_store_explicit(
    &(share_object)->header.ref_count,
    0, // the object is not yet referenced by anything
    memory_order_release // prevents later loads from seeing the pre-init value
  );
  atomic_store_explicit(
    &(share_object)->header.link,
    NULL, // "possibly dead" state, and ready to append to the end of the list
    memory_order_release // prevents later loads from seeing the pre-init value
  );
  append_to_possibly_dead_list(share_object, &local_share_state);

  // Set the type field.
  share_object->object.type = type;

  // The data fields are left uninitialized, because we don't know them here.
  // They are the responsibility of some downstream constructor to handle.

  // Return the object itself (the portion after the header).
  return &(share_object)->object;
}


void vecs_share_inc_ref(vecs_object_t* object) {
  VECS_ASSERT(object != NULL);
  share_object_t* share_object = get_share_object(object);

  // We can't modify the reference count of baked objects.
  if (is_share_object_actually_baked(share_object)) return;

  // Increment the reference count.
  size_t old_ref_count = atomic_fetch_add_explicit(
    &share_object->header.ref_count, 1, memory_order_acq_rel
  );
  VECS_ASSERT(
    old_ref_count != (SIZE_MAX - 1) /* no overflow into "baked" marker value */
  );

  return;
}


void vecs_share_dec_ref(vecs_object_t* object) {
  VECS_ASSERT(object != NULL);
  share_object_t* share_object = get_share_object(object);

  // We can't modify the reference count of baked objects.
  if (is_share_object_actually_baked(share_object)) return;

  // Decrement the reference count.
  size_t old_ref_count = atomic_fetch_sub_explicit(
    &share_object->header.ref_count, 1, memory_order_acq_rel
  );
  VECS_ASSERT(old_ref_count != 0 /* no underflow */);

  // If the reference count drops to zero, mark the object as "possibly dead".
  //
  // Note that it's possible another `vecs_share_inc_ref` could still happen
  // before reclaim (from some other in-flight reference in the program),
  // which is why we only consider this to be "possibly" dead.
  if (old_ref_count == 1) set_possibly_dead(share_object);

  return;
}


///
// Internal-only "Public" API (see header for docs)


void vecsint_share_reclaim_local_dead() {
  VECS_ASSERT(!debug_local_reclaim_in_progress);

#ifdef VECS_ASSERTS_ENABLED
  // Debug-only bookkeeping to track when we're in a reclaim phase.
  debug_local_reclaim_in_progress = true;
  atomic_fetch_add_explicit(
    &debug_reclaim_threads_in_progress, 1, memory_order_acq_rel
  );
#endif

  // One invariant here is that each "possibly dead" object returns to the list
  // associated with the thread that allocated it, and never to multiple lists.
  // Another invariant is that there is no concurrent "marking as possibly dead"
  // happening during a reclaim phase. Therefore we can modify our thread-local
  // share state and all of the "possibly dead" objects therein without
  // needing to use atomic operations or worrying about concurrent changes.
  // For the duration of this function, we have exclusive access to all that.

  // Prepare to track a list of objects that need to have their finalizers run.
  share_object_t** needs_fini_list_tail =
    &local_share_state.needs_fini_list_head;

  // For each object in the list of "possibly dead" objects for this thread,
  // each object will either be "actually dead" (still zero references) or it
  // will be "definitely live" (with one or more references).
  //
  // In both cases, we no longer need to track the object in the
  // "possibly dead" list anymore. Thus, the list can be trivially cleared,
  // without needing to go to the hassle of unlinking/relinking segments of it.
  for (
    share_object_t* share_object = local_share_state.possibly_dead_list_head;
    share_object != NULL;
    share_object = (share_object_t*)share_object->header.link
  ) {
    if (share_object->header.ref_count == 0) {
      // If the object still has a reference count of zero, it's now
      // "actually dead", and we can reclaim its memory for future allocations.
      // But first we need to run its finalizer function, if it has one.
      if (vecs_type_needs_fini(share_object->object.type)) {
        // BUT(!) We can't actually run the finalizer right now!
        // We're in the middle of a reclaim phase, and often the finalizer
        // will involve running `vecs_share_dec_ref` on other objects,
        // which is not allowed during a reclaim phase.
        //
        // So instead, we'll add it to the list of objects that need finalizing.
        // We'll run the finalizers for all of them after the reclaim phase.
        *needs_fini_list_tail = share_object;
        needs_fini_list_tail = (share_object_t**)&share_object->header.link;
      } else {
        // If no finalizer is needed, we can just free the object now.
        free_object(share_object);
      }
    } else {
      // Otherwise, the object is now "definitely live" and we can now
      // "return it to the wild" by marking its link as a reference
      // to the local share state (which will mark it as "definitely live"
      // and also ensure that it can refer back to the owning thread's state
      // when it gets marked as "possibly dead" again in the future).
      share_object->header.link = (void*)&local_share_state;
      // We leave its reference count unchanged at whatever nonzero value it is.
    }
  }

  // Clear the list - we're done tracking all those items now.
  // Each one is either reclaimed, or returned back to the wild.
  local_share_state.possibly_dead_list_head = NULL;

#ifdef VECS_ASSERTS_ENABLED
  // Debug-only bookkeeping to track when we're in a reclaim phase.
  atomic_fetch_sub_explicit(
    &debug_reclaim_threads_in_progress, 1, memory_order_acq_rel
  );
  debug_local_reclaim_in_progress = false;
#endif
}


void vecsint_share_run_local_finalizers() {
  VECS_ASSERT(
    // Finalizers should not be run during a reclaim phase.
    0 == atomic_load_explicit(
      &debug_reclaim_threads_in_progress, memory_order_acq_rel
    )
  );

  // Run the finalizers for all the objects that need them.
  for (
    share_object_t* share_object = local_share_state.needs_fini_list_head;
    share_object != NULL;
    share_object = (share_object_t*)share_object->header.link
  ) {
    vecs_object_fini(&(share_object->object));
  }

  // Free memory for all the objects that had finalizers run.
  //
  // We do this as a separate step, because we don't want to potentially
  // mess with any memory that the finalizers might still be using.
  //
  // TODO: Is this too cautious? Can we guarantee safety using one step?
  for (
    share_object_t* share_object = local_share_state.needs_fini_list_head;
    share_object != NULL;
    share_object = (share_object_t*)share_object->header.link
  ) {
    free_object(share_object);
  }

  // Clear the list.
  local_share_state.needs_fini_list_head = NULL;
}
